{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf M-IFEval\n",
        "!git clone git@github.com:ManuPassinato/M-IFEvalFork.git M-IFEval\n",
        "%cd M-IFEval\n",
        "\n",
        "# Instala√ß√£o de depend√™ncias\n",
        "!cd M-IFEval && pip install -q -r requirements.txt\n",
        "!pip install -q vllm==0.7.1 bitsandbytes==0.45.1 hf-transfer==0.1.9 langdetect janome ja_sentence_segmenter\n",
        "\n",
        "# Setup do Spacy e NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.getcwd())\n",
        "!touch instruction_utils/__init__.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0VO6cXpajiI",
        "outputId": "ea130964-144c-4984-a381-4f11b0f03918"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'M-IFEval'...\n",
            "remote: Enumerating objects: 734, done.\u001b[K\n",
            "remote: Counting objects: 100% (734/734), done.\u001b[K\n",
            "remote: Compressing objects: 100% (415/415), done.\u001b[K\n",
            "remote: Total 734 (delta 320), reused 722 (delta 313), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (734/734), 30.32 MiB | 22.55 MiB/s, done.\n",
            "Resolving deltas: 100% (320/320), done.\n",
            "/content/M-IFEval\n",
            "/bin/bash: line 1: cd: M-IFEval: No such file or directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile universal_inference.py\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import gc\n",
        "import sys\n",
        "import traceback\n",
        "from datasets import load_dataset\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "def run_model_inference(model_name):\n",
        "    print(f\"\\n[WORKER] Iniciando: {model_name}\")\n",
        "\n",
        "    # 1. Limpeza\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. Carregar Modelo\n",
        "    try:\n",
        "        print(f\"[WORKER] Carregando vLLM...\")\n",
        "        llm = LLM(\n",
        "            model=model_name,\n",
        "            trust_remote_code=True,\n",
        "            gpu_memory_utilization=0.90,\n",
        "            max_model_len=4096,\n",
        "            enforce_eager=True,\n",
        "            tensor_parallel_size=1,\n",
        "            device=\"cuda\"\n",
        "        )\n",
        "    except Exception:\n",
        "        print(\"‚ùå [ERRO FATAL] Falha ao carregar o modelo.\")\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "    sampling_params = SamplingParams(temperature=0.0, max_tokens=2048)\n",
        "\n",
        "    # 3. Identificar arquivo de dados (Usa o CLEAN se existir, sen√£o o normal)\n",
        "    data_dir = \"./data\"\n",
        "    input_file = None\n",
        "\n",
        "    # Prioridade para o arquivo limpo que geramos\n",
        "    if os.path.exists(os.path.join(data_dir, \"pt_input_data_FINAL_CLEAN.jsonl\")):\n",
        "        input_file = \"pt_input_data_FINAL_CLEAN.jsonl\"\n",
        "    elif os.path.exists(os.path.join(data_dir, \"pt_input_data_clean.jsonl\")):\n",
        "        input_file = \"pt_input_data_clean.jsonl\"\n",
        "    elif os.path.exists(os.path.join(data_dir, \"pt_input_data.jsonl\")):\n",
        "        input_file = \"pt_input_data.jsonl\"\n",
        "\n",
        "    if not input_file:\n",
        "        print(f\"‚ùå Nenhum arquivo de input encontrado em {data_dir}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    input_path = os.path.join(data_dir, input_file)\n",
        "    print(f\"[WORKER] Usando arquivo de entrada: {input_file}\")\n",
        "\n",
        "    # 4. Processamento\n",
        "    try:\n",
        "        ds = load_dataset(\"json\", data_files={\"train\": input_path}, split=\"train\")\n",
        "\n",
        "        # Detecta coluna de prompt\n",
        "        col_names = ds.column_names\n",
        "        prompt_col = \"prompt\"\n",
        "        if \"prompt\" not in col_names:\n",
        "            # Tenta achar substitutos\n",
        "            for c in [\"instruction\", \"pergunta\", \"input\"]:\n",
        "                if c in col_names:\n",
        "                    prompt_col = c; break\n",
        "\n",
        "        print(f\"[WORKER] Coluna de prompt detectada: '{prompt_col}'\")\n",
        "        prompts = [item[prompt_col] for item in ds]\n",
        "\n",
        "        # Gera√ß√£o\n",
        "        outputs = llm.generate(prompts, sampling_params)\n",
        "        generated_text = [output.outputs[0].text for output in outputs]\n",
        "\n",
        "        # Salva Sa√≠da\n",
        "        safe_model = model_name.replace('/', '__')\n",
        "        output_filename = os.path.join(data_dir, f\"pt_input_response_data_{safe_model}_new.jsonl\")\n",
        "\n",
        "        ds = ds.add_column(\"response\", generated_text)\n",
        "        ds.select_columns([prompt_col, \"response\"]).to_json(output_filename)\n",
        "        print(f\"‚úÖ [SUCESSO] Arquivo salvo: {output_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå [ERRO] Falha durante gera√ß√£o: {e}\")\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_name\", type=str, required=True)\n",
        "    args = parser.parse_args()\n",
        "    run_model_inference(args.model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNkQfduOfPjB",
        "outputId": "5f94994b-367b-4460-f672-c5f184318e2f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting universal_inference.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala os modelos lingu√≠sticos do Spacy necess√°rios para as 4 l√≠nguas\n",
        "print(\"Instalando modelos do Spacy para EN, ES, FR e JA...\")\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "!python -m spacy download ja_core_news_sm\n",
        "!python -m spacy download pt_core_news_sm\n",
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "print(\"\\n‚úÖ Instala√ß√£o conclu√≠da. Pronto para rodar o benchmark completo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqdloQXCalM0",
        "outputId": "9ea2e92a-e653-48bc-9218-f31316c734e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando modelos do Spacy para EN, ES, FR e JA...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ja-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_sm-3.8.0/ja_core_news_sm-3.8.0-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachipy!=0.6.1,>=0.5.2 (from ja-core-news-sm==3.8.0)\n",
            "  Downloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from ja-core-news-sm==3.8.0)\n",
            "  Downloading sudachidict_core-20251022-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading sudachidict_core-20251022-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sudachipy, sudachidict-core, ja-core-news-sm\n",
            "Successfully installed ja-core-news-sm-3.8.0 sudachidict-core-20251022 sudachipy-0.6.10\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ja_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-sent-ud-sm\n",
            "Successfully installed xx-sent-ud-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "‚úÖ Instala√ß√£o conclu√≠da. Pronto para rodar o benchmark completo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def rename_json_to_jsonl():\n",
        "    old_path = \"/content/M-IFEval/data/pt_input_data.json\"\n",
        "    new_path = \"/content/M-IFEval/data/pt_input_data.jsonl\"\n",
        "\n",
        "    if not os.path.exists(old_path):\n",
        "        print(f\"‚ùå Arquivo n√£o encontrado: {old_path}\")\n",
        "        return\n",
        "\n",
        "    if os.path.exists(new_path):\n",
        "        print(f\"‚ö†Ô∏è O arquivo j√° existe: {new_path}\")\n",
        "        return\n",
        "\n",
        "    os.rename(old_path, new_path)\n",
        "    print(\"‚úÖ Arquivo renomeado com sucesso.\")\n",
        "\n",
        "rename_json_to_jsonl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2kif4eLuyVb",
        "outputId": "4aa45996-4157-4fea-ed50-e76fee028e1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Arquivo renomeado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "retirei os arquivos com \"pt:detectable_format:constrained_response\" do \"data/pt_input_data.jsonl\", pois estavam dando problema ao rodar o benchmark"
      ],
      "metadata": {
        "id": "Y2q0PjF7-4j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "print(\"üßπ INICIANDO LIMPEZA CIR√öRGICA...\")\n",
        "\n",
        "# Lista negra exata baseada no seu erro\n",
        "KILL_LIST = [\n",
        "    \"pt:detectable_format:constrained_response\",\n",
        "]\n",
        "\n",
        "input_path = \"data/pt_input_data.jsonl\"\n",
        "output_path = \"data/pt_input_data_FINAL_CLEAN.jsonl\"\n",
        "\n",
        "total = 0\n",
        "kept = 0\n",
        "removed = 0\n",
        "\n",
        "if not os.path.exists(input_path):\n",
        "    print(\"‚ùå Arquivo original n√£o encontrado!\")\n",
        "else:\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as fin, \\\n",
        "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "\n",
        "        for line in fin:\n",
        "            total += 1\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                ids = data.get(\"instruction_id_list\", [])\n",
        "\n",
        "                # Verifica se ALGUM dos IDs pedidos est√° na lista negra\n",
        "                # any() retorna True se encontrar qualquer correspond√™ncia\n",
        "                is_bad_line = any(bad_id in ids for bad_id in KILL_LIST)\n",
        "\n",
        "                if is_bad_line:\n",
        "                    removed += 1\n",
        "                    # print(f\"   üóëÔ∏è Removendo linha com: {ids}\")\n",
        "                else:\n",
        "                    fout.write(line)\n",
        "                    kept += 1\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üìä RESULTADO:\")\n",
        "    print(f\"   Total lido:      {total}\")\n",
        "    print(f\"   Linhas APAGADAS: {removed}\")\n",
        "    print(f\"   Linhas MANTIDAS: {kept}\")\n",
        "    print(f\"   Arquivo salvo:   {output_path}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if kept > 0:\n",
        "        print(\"‚úÖ Arquivo limpo gerado com sucesso.\")\n",
        "    else:\n",
        "        print(\"‚ùå ALERTA: O arquivo resultante ficou vazio.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ro--kyqBFz",
        "outputId": "b77aca9e-c3bf-4933-d3ff-287d3b162ed4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ INICIANDO LIMPEZA CIR√öRGICA...\n",
            "----------------------------------------\n",
            "üìä RESULTADO:\n",
            "   Total lido:      129\n",
            "   Linhas APAGADAS: 3\n",
            "   Linhas MANTIDAS: 126\n",
            "   Arquivo salvo:   data/pt_input_data_FINAL_CLEAN.jsonl\n",
            "----------------------------------------\n",
            "‚úÖ Arquivo limpo gerado com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muda o arquivo instruction_utils/pt_instructions_util.py\n",
        "\n",
        "Adiciona \"nlp.add_pipe(\"sentencizer\")\" na fun√ß√£o     _get_sentence_tokenizer()\n",
        "\n",
        "Antes o parser era desativado na fun√ß√£o, mas no spaCy.sents depende do parser OU de um sentencizer.\n"
      ],
      "metadata": {
        "id": "EfMjxd9d5Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instruction_utils/pt_instructions_util.py\n",
        "# coding=utf-8\n",
        "# Copyright 2024 The Google Research Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Utility library of instructions.\"\"\"\n",
        "\n",
        "import functools\n",
        "import random\n",
        "import re\n",
        "from typing import List\n",
        "import spacy\n",
        "import immutabledict\n",
        "import nltk\n",
        "\n",
        "WORD_LIST = [\"saudade\", \"amanh√£\", \"cidad√£o\", \"trabalho\", \"escola\", \"sa√∫de\", \"privado\", \"justi√ßa\", \"cultura\", \"verdade\"]  # pylint: disable=line-too-long\n",
        "\n",
        "# ISO 639-1 codes to language names.\n",
        "LANGUAGE_CODES = immutabledict.immutabledict({\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"fr\": \"French\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"de\": \"German\",\n",
        "    \"ja\": \"Japanese\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"bn\": \"Bengali\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    \"th\": \"Thai\",\n",
        "    \"ur\": \"Urdu\",\n",
        "    \"ta\": \"Tamil\",\n",
        "    \"te\": \"Telugu\",\n",
        "    \"bg\": \"Bulgarian\",\n",
        "    \"ko\": \"Korean\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"he\": \"Hebrew\",\n",
        "    \"fa\": \"Persian\",\n",
        "    \"vi\": \"Vietnamese\",\n",
        "    \"ne\": \"Nepali\",\n",
        "    \"sw\": \"Swahili\",\n",
        "    \"kn\": \"Kannada\",\n",
        "    \"mr\": \"Marathi\",\n",
        "    \"gu\": \"Gujarati\",\n",
        "    \"pa\": \"Punjabi\",\n",
        "    \"ml\": \"Malayalam\",\n",
        "    \"fi\": \"Finnish\",\n",
        "    })\n",
        "\n",
        "_ALPHABETS = \"([A-Za-z√Å-√ö√°-√∫√Ä-√π√¢√™√Æ√¥√ª√É-√ï√£-√µ√á√ß])\"\n",
        "_PREFIXES = \"(Sr|Sra|Srta|Dr|Dra|Prof|Profa)[.]\"\n",
        "_SUFFIXES = \"(Ltda|ME|SA|Jr|Filho|Neto|Co)\"\n",
        "_STARTERS = r\"(Sr|Sra|Srta|Dr|Dra|Prof|Ele\\s|Ela\\s|Eles\\s|Elas\\s|Isso\\s|Aquilo\\s|Aquele\\s|Aquela\\s|Mas\\s|Por√©m\\s|Contudo\\s|Entretanto\\s|Assim\\s|Ent√£o\\s|Onde\\s|Quando\\s|Enquanto\\s|Se\\s|Caso\\s)\"\n",
        "_ACRONYMS = \"([A-Z√Å-√ö][.][A-Z√Å-√ö][.](?:[A-Z√Å-√ö][.])?)\"\n",
        "_WEBSITES = \"[.](com|net|org|io|gov|edu|me|br)\"\n",
        "_DIGITS = \"([0-9])\"\n",
        "_MULTIPLE_DOTS = r\"\\.{2,}\"\n",
        "\n",
        "def split_into_sentences(text):\n",
        "  \"\"\"Split the text into sentences.\n",
        "\n",
        "  Args:\n",
        "    text: A string that consists of more than or equal to one sentences.\n",
        "\n",
        "  Returns:\n",
        "    A list of strings where each string is a sentence.\n",
        "  \"\"\"\n",
        "  text = \" \" + text + \"  \"\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  text = re.sub(_PREFIXES, \"\\\\1<prd>\", text)\n",
        "  text = re.sub(_WEBSITES, \"<prd>\\\\1\", text)\n",
        "  text = re.sub(_DIGITS + \"[.]\" + _DIGITS, \"\\\\1<prd>\\\\2\", text)\n",
        "  text = re.sub(\n",
        "      _MULTIPLE_DOTS,\n",
        "      lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\",\n",
        "      text,\n",
        "  )\n",
        "  if \"Ph.D\" in text:\n",
        "    text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
        "  text = re.sub(r\"\\s\" + _ALPHABETS + \"[.] \", \" \\\\1<prd> \", text)\n",
        "  text = re.sub(_ACRONYMS + \" \" + _STARTERS, \"\\\\1<stop> \\\\2\", text)\n",
        "  text = re.sub(\n",
        "      _ALPHABETS + \"[.]\" + _ALPHABETS + \"[.]\" + _ALPHABETS + \"[.]\",\n",
        "      \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",\n",
        "      text,\n",
        "  )\n",
        "  text = re.sub(\n",
        "      _ALPHABETS + \"[.]\" + _ALPHABETS + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text\n",
        "  )\n",
        "  text = re.sub(\" \" + _SUFFIXES + \"[.] \" + _STARTERS, \" \\\\1<stop> \\\\2\", text)\n",
        "  text = re.sub(\" \" + _SUFFIXES + \"[.]\", \" \\\\1<prd>\", text)\n",
        "  text = re.sub(\" \" + _ALPHABETS + \"[.]\", \" \\\\1<prd>\", text)\n",
        "  if \"‚Äù\" in text:\n",
        "    text = text.replace(\".‚Äù\", \"‚Äù.\")\n",
        "  if '\"' in text:\n",
        "    text = text.replace('.\"', '\".')\n",
        "  if \"!\" in text:\n",
        "    text = text.replace('!\"', '\"!')\n",
        "  if \"?\" in text:\n",
        "    text = text.replace('?\"', '\"?')\n",
        "  text = text.replace(\".\", \".<stop>\")\n",
        "  text = text.replace(\"?\", \"?<stop>\")\n",
        "  text = text.replace(\"!\", \"!<stop>\")\n",
        "  text = text.replace(\"<prd>\", \".\")\n",
        "  sentences = text.split(\"<stop>\")\n",
        "  sentences = [s.strip() for s in sentences]\n",
        "  if sentences and not sentences[-1]:\n",
        "    sentences = sentences[:-1]\n",
        "  return sentences\n",
        "\n",
        "def count_words(text):\n",
        "  \"\"\"Counts the number of words.\"\"\"\n",
        "  nlp = _get_sentence_tokenizer()\n",
        "  tokenized_text = nlp(text)  # Process the text with the Portuguese tokenizer\n",
        "  num_words = len([token.text for token in tokenized_text if not token.is_punct])  # Count non-punctuation tokens\n",
        "  return num_words\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def _get_sentence_tokenizer():\n",
        "    # --- CORRE√á√ÉO APLICADA AQUI ---\n",
        "    # Carregamos o modelo desabilitando componentes pesados\n",
        "    nlp = spacy.load(\"pt_core_news_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
        "    # Adicionamos manualmente o 'sentencizer' para permitir a divis√£o de frases (.sents)\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "    return nlp\n",
        "\n",
        "def tokenize_words(text):\n",
        "  \"\"\"Returns a list of words from the text, respecting Portuguese special characters and features with spaCy.\"\"\"\n",
        "  # Load the Portuguese tokenizer model from spaCy\n",
        "  nlp = _get_sentence_tokenizer()\n",
        "  tokenized_text = nlp(text)  # Process the text with the Portuguese tokenizer\n",
        "  # Extract non-punctuation tokens\n",
        "  words = [token.text for token in tokenized_text if not token.is_punct]\n",
        "  return words\n",
        "\n",
        "def count_sentences(text) -> int:\n",
        "    nlp = _get_sentence_tokenizer()\n",
        "    tokenized_text = nlp(text)\n",
        "    num_sentences = len(list(tokenized_text.sents))  # Count the number of sentences\n",
        "    return num_sentences\n",
        "\n",
        "def generate_keywords(num_keywords):\n",
        "  \"\"\"Randomly generates a few keywords.\"\"\"\n",
        "  return random.sample(WORD_LIST, k=num_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFanKlGE4fOR",
        "outputId": "ddbbc919-f24a-441c-b1bf-1f3ea8700eda"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting instruction_utils/pt_instructions_util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "from huggingface_hub import scan_cache_dir\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- CONFIGURA√á√ÉO DA ESCALA ---\n",
        "MODELS_TO_BENCHMARK = [\n",
        "    # 'gpt-4o-mini-2024-07-18',\n",
        "    # 'gpt-4o-2024-08-06',\n",
        "    # 'o1-preview-2024-09-12',\n",
        "    # 'o1-mini-2024-09-12',\n",
        "    # 'claude-3-haiku-20240307',\n",
        "    # 'claude-3-5-sonnet-20240620',\n",
        "    # 'claude-3-opus-20240229',\n",
        "    # 'gemini-1.5-pro-002',\n",
        "    # 'gemini-1.5-flash-002',\n",
        "    # 'CohereForAI/c4ai-command-r-plus-4bit',\n",
        "    # 'CohereForAI/c4ai-command-r-v01-4bit',\n",
        "    # 'CohereForAI/aya-23-8B',\n",
        "     'Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4',\n",
        "    # 'Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4'',\n",
        "    # 'hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4',\n",
        "    # 'hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4',\n",
        "    # 'mistralai/Mistral-7B-Instruct-v0.3',\n",
        "    # 'deepseek-ai/deepseek-llm-7b-chat''\n",
        "]\n",
        "\n",
        "def delete_model_cache(model_id):\n",
        "    print(f\"üßπ Limpando cache para liberar espa√ßo: {model_id}...\")\n",
        "    try:\n",
        "        hf_cache_info = scan_cache_dir()\n",
        "        found = False\n",
        "        for repo in hf_cache_info.repos:\n",
        "            if repo.repo_id == model_id:\n",
        "                shutil.rmtree(repo.repo_path)\n",
        "                found = True\n",
        "        if found: print(\"   -> Cache removido.\")\n",
        "        else: print(\"   -> Nada no cache para remover.\")\n",
        "    except Exception as e:\n",
        "        print(f\"   -> Erro n√£o fatal na limpeza: {e}\")\n",
        "\n",
        "def format_time(seconds):\n",
        "    return str(timedelta(seconds=int(seconds)))\n",
        "\n",
        "# --- IN√çCIO ---\n",
        "benchmark_start_time = time.time()\n",
        "\n",
        "for model in MODELS_TO_BENCHMARK:\n",
        "    model_start_time = time.time()\n",
        "    safe_model_name = model.replace('/', '__')\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"üöÄ INICIANDO: {model}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # --- PASSO 1: INFER√äNCIA ---\n",
        "    print(\">> Passo 1: Infer√™ncia (Processo Isolado)\")\n",
        "    t0_inf = time.time()\n",
        "\n",
        "    try:\n",
        "        # check=True faz o Python disparar erro se o worker retornar c√≥digo != 0\n",
        "        # Isso acontece se der OOM ou crash no script filho\n",
        "        process = subprocess.run(\n",
        "            [\"python\", \"universal_inference.py\", \"--model_name\", model],\n",
        "            check=True,\n",
        "            text=True\n",
        "        )\n",
        "        inference_time = time.time() - t0_inf\n",
        "        print(f\"   ‚è±Ô∏è Tempo de Infer√™ncia: {format_time(inference_time)}\")\n",
        "        inferencia_sucesso = True\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\n‚ùå FALHA NO MODELO {model}\")\n",
        "        print(f\"   Motivo: O processo de infer√™ncia retornou erro (C√≥digo {e.returncode}).\")\n",
        "        print(\"   Diagn√≥stico: Provavelmente falta de mem√≥ria (OOM) ou erro de biblioteca.\")\n",
        "        print(\"   A√ß√£o: Pulando avalia√ß√£o deste modelo e limpando recursos.\")\n",
        "\n",
        "        # Limpa e vai para o pr√≥ximo loop\n",
        "        delete_model_cache(model)\n",
        "        inferencia_sucesso = False\n",
        "        continue # <--- Se quiser pular a avalia√ß√£o desse modelo espec√≠fico\n",
        "\n",
        "    # --- PASSO 2: AVALIA√á√ÉO (S√≥ roda se a infer√™ncia funcionou) ---\n",
        "    if inferencia_sucesso:\n",
        "        print(\"\\n>> Passo 2: Avalia√ß√£o\")\n",
        "        t0_eval = time.time()\n",
        "        langs = [\"pt\"]\n",
        "\n",
        "        for lang in langs:\n",
        "            resp_file = f\"data/{lang}_input_response_data_{safe_model_name}_new.jsonl\"\n",
        "            out_dir = f\"evaluations/{lang}_input_response_data_{safe_model_name}_new\"\n",
        "\n",
        "            if os.path.exists(resp_file):\n",
        "                os.makedirs(out_dir, exist_ok=True)\n",
        "                try:\n",
        "                    subprocess.run([\n",
        "                        \"python\", \"-m\", \"evaluation_main\",\n",
        "                        \"--input_data\", f\"data/{lang}_input_data_FINAL_CLEAN.jsonl\",\n",
        "                        \"--input_response_data\", resp_file,\n",
        "                        \"--output_dir\", out_dir\n",
        "                    ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
        "                    print(f\"   ‚úÖ {lang.upper()}: OK\")\n",
        "                except:\n",
        "                    print(f\"   ‚ùå {lang.upper()}: Falhou na etapa de c√°lculo de m√©tricas.\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è {lang.upper()}: Arquivo de resposta n√£o encontrado.\")\n",
        "\n",
        "        print(f\"   ‚è±Ô∏è Tempo Avalia√ß√£o: {format_time(time.time() - t0_eval)}\")\n",
        "\n",
        "    # --- LIMPEZA FINAL DO CICLO ---\n",
        "    print(f\"\\n>> Passo 3: Limpeza P√≥s-Ciclo\")\n",
        "    delete_model_cache(model)\n",
        "\n",
        "    total_model_time = time.time() - model_start_time\n",
        "    print(f\"‚úÖ Ciclo finalizado para {model}\")\n",
        "    print(f\"‚è±Ô∏è Tempo total deste modelo: {format_time(total_model_time)}\\n\")\n",
        "\n",
        "# --- FIM GERAL ---\n",
        "total_benchmark_time = time.time() - benchmark_start_time\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"üéâ BENCHMARK COMPLETO! Tempo total: {format_time(total_benchmark_time)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnkmpKscaojO",
        "outputId": "4f53d82a-0796-4de5-87fc-8dc3cfa526eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ INICIANDO: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "============================================================\n",
            ">> Passo 1: Infer√™ncia (Processo Isolado)\n",
            "   ‚è±Ô∏è Tempo de Infer√™ncia: 0:03:00\n",
            "\n",
            ">> Passo 2: Avalia√ß√£o\n",
            "   ‚úÖ PT: OK\n",
            "   ‚è±Ô∏è Tempo Avalia√ß√£o: 0:00:24\n",
            "\n",
            ">> Passo 3: Limpeza P√≥s-Ciclo\n",
            "üßπ Limpando cache para liberar espa√ßo: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4...\n",
            "   -> Cache removido.\n",
            "‚úÖ Ciclo finalizado para Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "‚è±Ô∏è Tempo total deste modelo: 0:03:25\n",
            "\n",
            "\n",
            "============================================================\n",
            "üéâ BENCHMARK COMPLETO! Tempo total: 0:03:25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BAIXA OS JSONs\n",
        "\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# Usa a mesma l√≥gica de nome do modelo\n",
        "if 'MODELS_TO_BENCHMARK' in globals() and MODELS_TO_BENCHMARK:\n",
        "    model_name = MODELS_TO_BENCHMARK[0]\n",
        "else:\n",
        "    # Fallback caso a vari√°vel n√£o esteja na mem√≥ria\n",
        "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\"\n",
        "\n",
        "safe_model_name = model_name.replace('/', '__')\n",
        "\n",
        "# Cria um nome √∫nico para o zip\n",
        "timestamp = datetime.now().strftime(\"%H%M\")\n",
        "nome_zip = f\"novos_jsons_{safe_model_name}_{timestamp}\"\n",
        "pasta_temp = \"download_temp_jsons\"\n",
        "\n",
        "# Cria a pasta tempor√°ria para organizar os arquivos\n",
        "os.makedirs(pasta_temp, exist_ok=True)\n",
        "\n",
        "print(f\"üì¶ Preparando pacote de download para: {model_name}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "arquivos_zipados = 0\n",
        "langs = [\"pt\"]\n",
        "\n",
        "for lang in langs:\n",
        "    # O caminho exato dos arquivos \"_new\"\n",
        "    file_path = f\"data/{lang}_input_response_data_{safe_model_name}_new.jsonl\"\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        # Copia o arquivo para a pasta de download\n",
        "        shutil.copy(file_path, pasta_temp)\n",
        "        print(f\"‚úÖ Adicionado ao pacote: {os.path.basename(file_path)}\")\n",
        "        arquivos_zipados += 1\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Arquivo n√£o encontrado (n√£o ser√° baixado): {file_path}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if arquivos_zipados > 0:\n",
        "    print(f\"üìö Compactando {arquivos_zipados} arquivos...\")\n",
        "    shutil.make_archive(nome_zip, 'zip', pasta_temp)\n",
        "\n",
        "    print(f\"‚¨áÔ∏è Iniciando download de {nome_zip}.zip ...\")\n",
        "    files.download(f\"{nome_zip}.zip\")\n",
        "\n",
        "    # Limpa a pasta tempor√°ria ap√≥s gerar o zip (opcional)\n",
        "    shutil.rmtree(pasta_temp)\n",
        "else:\n",
        "    print(\"‚ùå Nenhum arquivo encontrado para baixar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "QD0X5E798BdX",
        "outputId": "adc557fe-a5b1-4f17-c4d7-edfb3e188b5c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Preparando pacote de download para: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "------------------------------------------------------------\n",
            "‚úÖ Adicionado ao pacote: pt_input_response_data_Qwen__Qwen2.5-0.5B-Instruct-GPTQ-Int4_new.jsonl\n",
            "------------------------------------------------------------\n",
            "üìö Compactando 1 arquivos...\n",
            "‚¨áÔ∏è Iniciando download de novos_jsons_Qwen__Qwen2.5-0.5B-Instruct-GPTQ-Int4_0133.zip ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f3f630f-e49a-40d4-99c2-2c2a9c45714f\", \"novos_jsons_Qwen__Qwen2.5-0.5B-Instruct-GPTQ-Int4_0133.zip\", 40593)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BAIXA OS EVALUATIONS\n",
        "\n",
        "# Tenta pegar o nome do modelo da vari√°vel global, ou usa o padr√£o se n√£o existir\n",
        "if 'MODELS_TO_BENCHMARK' in globals() and MODELS_TO_BENCHMARK:\n",
        "    model_name = MODELS_TO_BENCHMARK[0]\n",
        "else:\n",
        "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\"\n",
        "\n",
        "safe_model_name = model_name.replace('/', '__')\n",
        "\n",
        "# Cria nome √∫nico para o zip\n",
        "timestamp = datetime.now().strftime(\"%H%M\")\n",
        "nome_zip = f\"resultados_evaluation_new_{timestamp}\"\n",
        "pasta_temp = \"download_temp_evals\"\n",
        "\n",
        "# Garante que a pasta tempor√°ria est√° limpa/criada\n",
        "if os.path.exists(pasta_temp):\n",
        "    shutil.rmtree(pasta_temp)\n",
        "os.makedirs(pasta_temp)\n",
        "\n",
        "print(f\"üì¶ Empacotando resultados de avalia√ß√£o para: {model_name}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "pastas_encontradas = 0\n",
        "langs = [\"pt\"]\n",
        "\n",
        "for lang in langs:\n",
        "    # O caminho exato que o seu script anterior verificou\n",
        "    src_dir = f\"evaluations/{lang}_input_response_data_{safe_model_name}_new\"\n",
        "\n",
        "    # Define onde salvar dentro do zip (ex: download_temp/en_results)\n",
        "    dst_dir = os.path.join(pasta_temp, f\"{lang}_results\")\n",
        "\n",
        "    if os.path.exists(src_dir):\n",
        "        # Copia a pasta inteira (com os jsons strict e loose dentro)\n",
        "        shutil.copytree(src_dir, dst_dir)\n",
        "        print(f\"‚úÖ Adicionada pasta: {src_dir}\")\n",
        "        pastas_encontradas += 1\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Pasta n√£o encontrada (ignorada): {src_dir}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if pastas_encontradas > 0:\n",
        "    print(f\"üìö Compactando {pastas_encontradas} pastas de avalia√ß√£o...\")\n",
        "    shutil.make_archive(nome_zip, 'zip', pasta_temp)\n",
        "\n",
        "    print(f\"‚¨áÔ∏è Iniciando download de {nome_zip}.zip ...\")\n",
        "    files.download(f\"{nome_zip}.zip\")\n",
        "\n",
        "    # Limpeza\n",
        "    shutil.rmtree(pasta_temp)\n",
        "else:\n",
        "    print(\"‚ùå Nenhuma pasta de avalia√ß√£o encontrada para baixar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "xCF-L0FA8Hka",
        "outputId": "e6cf0fa3-1b37-4557-9a46-5cf8f361e413"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Empacotando resultados de avalia√ß√£o para: Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\n",
            "------------------------------------------------------------\n",
            "‚úÖ Adicionada pasta: evaluations/pt_input_response_data_Qwen__Qwen2.5-0.5B-Instruct-GPTQ-Int4_new\n",
            "------------------------------------------------------------\n",
            "üìö Compactando 1 pastas de avalia√ß√£o...\n",
            "‚¨áÔ∏è Iniciando download de resultados_evaluation_new_0133.zip ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2b1550ae-6c53-4b81-9737-91830e895977\", \"resultados_evaluation_new_0133.zip\", 88050)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}